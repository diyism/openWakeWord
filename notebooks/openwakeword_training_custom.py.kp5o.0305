# -*- coding: utf-8 -*-
"""openwakeword_automatic_model_training_simple.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gDfd3kcsxgrHxHCJsz8rE7_d9Up9VQhF
"""
import sys,os
from google.colab import drive
drive.mount('/content/gdrive', force_remount=True)
dir_cut_syl="/content/gdrive/MyDrive/ColabData/openWakeWord/cut_syl_data"
!ls -dal $dir_cut_syl

!ls -al $dir_cut_syl | wc -l

#sys.exit("Stopping to check dir_cut_syl contents") # 调试代码已注释

#====begin: mount /root/gdrive/MyDrive/ColabSingularity/sing_posix_boxes/env1
!apt install fuse-posixovl
!umount -f /content/sing_posix_boxes
!mkdir /content/sing_posix_boxes
!mount.posixovl /root/gdrive/MyDrive/ColabSingularity/sing_posix_boxes -F /content/sing_posix_boxes
!df -h
!type python3
#first time, make env1
#!rm -rf /content/sing_posix_boxes/env1
#!python3 -m venv --system-site-packages --without-pip /content/sing_posix_boxes/env1
#!curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py
#!/content/sing_posix_boxes/env1/bin/python get-pip.py
#!rm get-pip.py
#!source ./env1/bin/activate; type python
#!type python
import os
venv_bin_path = '/content/sing_posix_boxes/env1/bin'
original_path = os.environ.get('PATH', '')
if not venv_bin_path in original_path.split(os.pathsep):
    os.environ['PATH'] = f"{venv_bin_path}{os.pathsep}{original_path}"
!type python
#====end: mount /root/gdrive/MyDrive/ColabSingularity/sing_posix_boxes/env1

# "Edit / Notebook settings / Omit code cell output when saving this notebook", then "File / Download / Download .ipynb"
!rm -rf my_custom_model/hey*
!rm -rf my_custom_model/heh*
!ls
#!pip uninstall torchaudio
!find . -type d -name '__pycache__' -exec rm -rf {} +
!pip install torch==2.5.1 torchvision==0.20.1+cu121 torchaudio==2.5.1+cu121 --index-url https://download.pytorch.org/whl/cu121
!pip list | grep -E "torch|torchaudio|piper|webrtcvad"
!nvcc --version

import torch
print("CUDA available:", torch.cuda.is_available())
print("PyTorch CUDA version:", torch.version.cuda)
print("Current device:", torch.cuda.current_device())
print("Device name:", torch.cuda.get_device_name(0))

import torchaudio
print(torchaudio.__file__)
print("TorchAudio version:", torchaudio.__version__)
print("CUDA version from TorchAudio:", torchaudio.lib._torchaudio.cuda_version())

"""## Training your own openWakeWord models

**Quick-start:** If you just want to train a basic custom model for openWakeWord!

Follow the instructions for Step 1 below. Each time you change the wake word, click the play icon to the left of the title to generate a sample and make sure it sounds correct. The first time it takes ~30 seconds but subsequent runs will be quick.

Once you're satisfied with the pronounciation, go to the "Runtime" dropdown menu in the upper left of the page, and select "run all". Keep the tab open but feel free to do something else. After ~1 hour, your custom model will be ready and will automatically be downloaded to your computer!

If you are a Home Assistant user with the openWakeWord add-on, follow the instructions [here](https://github.com/home-assistant/addons/blob/master/openwakeword/DOCS.md#custom-wake-word-models) to install and enable your custom model.

---

If you are interested in learning more about the custom model training process (and increasing the accuracy of your custom models), read through each step in this notebook and try experimenting with different training parameters. If you have any questions or problems, feel free to start a discussion at the openWakeWord [repo](https://github.com/dscripka/openWakeWord/discussions).
"""

# @title  { display-mode: "form" }
# @markdown # 1. Define Target Wake Word (for model name/path)
# @markdown Type in your target wake word below. This will be used to name
# @markdown the output model and organize the data directories.
# @markdown **Note:** We are no longer generating audio from this text.
# @markdown We will copy pre-existing positive audio files instead.
# @markdown
# @markdown Example: `eigh`, `hello_robot`

# @markdown - Spell out numbers ("2" --> "two")
# @markdown - Avoid punctuation except "_"
from IPython.display import Audio
if not os.path.exists("./piper-sample-generator"):
    !git clone https://github.com/rhasspy/piper-sample-generator
!pip install piper-phonemize
!pip install webrtcvad
!pip install onnxsim
#needed by train.py, even if won't use this:
if "piper-sample-generator/" not in sys.path:
    sys.path.append("piper-sample-generator/")
#from generate_samples import generate_samples
# @markdown - Use underscores "_" instead of spaces.

target_word = 'eigh' # @param {type:"string"}

# @title  { display-mode: "form" }
# @markdown # 2. Download Data & Prepare Positive Examples
# @markdown Training custom models requires downloading a wide variety of data
# @markdown that will help make the model perform well in real-world scenarios.
# @markdown This step downloads background noise, music, Room Impulse Responses (RIRs),
# @markdown and pre-computed features. It also copies your positive example
# @markdown audio files from the `dir_cut_syl` directory defined earlier.
# @markdown
# @markdown Downloading this example data usually takes about 15 minutes.
# @markdown Copying local data depends on the number of files.

# @markdown **Important note!** The data downloaded here has a mixture of difference
# @markdown licenses and usage restrictions. As such, any custom models trained with this
# @markdown data should be considered as appropriate for **non-commercial** personal use only.

# ## Install all dependencies
# !pip install datasets
# !pip install scipy
# !pip install tqdm

import locale
def getpreferredencoding(do_setlocale = True):
    return "UTF-8"
locale.getpreferredencoding = getpreferredencoding

# install openwakeword (full installation to support training)
!git clone https://github.com/diyism/openwakeword
!pip install -e ./openwakeword
!cd openwakeword

# install other dependencies
!pip install mutagen==1.47.0
!pip install torchinfo==1.8.0
!pip install torchmetrics==1.2.0
!pip install speechbrain==0.5.14
!pip install audiomentations==0.33.0
!pip install torch-audiomentations==0.11.0
!pip install acoustics==0.2.6
# !pip uninstall tensorflow -y
# !pip install tensorflow-cpu==2.8.1
# !pip install protobuf==3.20.3
# !pip install tensorflow_probability==0.16.0
# !pip install onnx_tf==1.10.0
!pip install onnx2tf
!pip install onnx
!pip install onnx_graphsurgeon
!pip install sng4onnx
!pip install pronouncing==0.2.0
!pip install datasets==2.14.6
!pip install deep-phonemizer==0.0.19

print("Installing dependencies for model conversion...")
!pip install onnx-tf # 满足 train.py 内部转换函数的需求
!pip install ai-edge-litert # 满足 onnx2tf 命令行的需求
print("Model conversion dependencies installation attempted.")

# Download required models (workaround for Colab)
#os.makedirs("./openwakeword/openwakeword/resources/models")
!wget https://github.com/dscripka/openWakeWord/releases/download/v0.5.1/embedding_model.onnx -O ./openwakeword/openwakeword/resources/models/embedding_model.onnx
!wget https://github.com/dscripka/openWakeWord/releases/download/v0.5.1/embedding_model.tflite -O ./openwakeword/openwakeword/resources/models/embedding_model.tflite
!wget https://github.com/dscripka/openWakeWord/releases/download/v0.5.1/melspectrogram.onnx -O ./openwakeword/openwakeword/resources/models/melspectrogram.onnx
!wget https://github.com/dscripka/openWakeWord/releases/download/v0.5.1/melspectrogram.tflite -O ./openwakeword/openwakeword/resources/models/melspectrogram.tflite


import numpy as np
import torch
from pathlib import Path
import uuid
import yaml
import datasets
import scipy
from tqdm import tqdm
import glob
import random
import shutil
import math

## Download all data

## Download MIR RIR data (takes about ~2 minutes)
output_dir = "./mit_rirs"
if not os.path.exists(output_dir):
    os.mkdir(output_dir)
    !git lfs install
    !git clone https://huggingface.co/datasets/davidscripka/MIT_environmental_impulse_responses
    rir_dataset = datasets.Dataset.from_dict({"audio": [str(i) for i in Path("./MIT_environmental_impulse_responses/16khz").glob("*.wav")]}).cast_column("audio", datasets.Audio())
    # Save clips to 16-bit PCM wav files
    for row in tqdm(rir_dataset):
        name = row['audio']['path'].split('/')[-1]
        scipy.io.wavfile.write(os.path.join(output_dir, name), 16000, (row['audio']['array']*32767).astype(np.int16))

## Download noise and background audio (takes about ~3 minutes)

# Audioset Dataset (https://research.google.com/audioset/dataset/index.html)
# Download one part of the audioset .tar files, extract, and convert to 16khz
# For full-scale training, it's recommended to download the entire dataset from
# https://huggingface.co/datasets/agkphysics/AudioSet, and
# even potentially combine it with other background noise datasets (e.g., FSD50k, Freesound, etc.)

if not os.path.exists("audioset"):
    os.mkdir("audioset")

    fname = "bal_train09.tar"
    out_dir = f"audioset/{fname}"
    link = "https://huggingface.co/datasets/agkphysics/AudioSet/resolve/main/data/" + fname
    !wget -O {out_dir} {link}
    !cd audioset && tar -xvf bal_train09.tar

    output_dir = "./audioset_16k"
    if not os.path.exists(output_dir):
        os.mkdir(output_dir)

    # Save clips to 16-bit PCM wav files
    audioset_dataset = datasets.Dataset.from_dict({"audio": [str(i) for i in Path("audioset/audio").glob("**/*.flac")]})
    audioset_dataset = audioset_dataset.cast_column("audio", datasets.Audio(sampling_rate=16000))
    for row in tqdm(audioset_dataset):
        name = row['audio']['path'].split('/')[-1].replace(".flac", ".wav")
        scipy.io.wavfile.write(os.path.join(output_dir, name), 16000, (row['audio']['array']*32767).astype(np.int16))

# Free Music Archive dataset
# https://github.com/mdeff/fma

output_dir = "./fma"
if not os.path.exists(output_dir):
    os.mkdir(output_dir)
    fma_dataset = datasets.load_dataset("rudraml/fma", name="small", split="train", streaming=True)
    fma_dataset = iter(fma_dataset.cast_column("audio", datasets.Audio(sampling_rate=16000)))

    # Save clips to 16-bit PCM wav files
    n_hours = 1  # use only 1 hour of clips for this example notebook, recommend increasing for full-scale training
    for i in tqdm(range(n_hours*3600//30)):  # this works because the FMA dataset is all 30 second clips
        row = next(fma_dataset)
        name = row['audio']['path'].split('/')[-1].replace(".mp3", ".wav")
        scipy.io.wavfile.write(os.path.join(output_dir, name), 16000, (row['audio']['array']*32767).astype(np.int16))
        i += 1
        if i == n_hours*3600//30:
            break

# Download pre-computed openWakeWord features for training and validation

# training set (~2,000 hours from the ACAV100M Dataset)
# See https://huggingface.co/datasets/davidscripka/openwakeword_features for more information
if not os.path.exists("./openwakeword_features_ACAV100M_2000_hrs_16bit.npy"):
    !wget https://huggingface.co/datasets/davidscripka/openwakeword_features/resolve/main/openwakeword_features_ACAV100M_2000_hrs_16bit.npy

# validation set for false positive rate estimation (~11 hours)
if not os.path.exists("validation_set_features.npy"):
    !wget https://huggingface.co/datasets/davidscripka/openwakeword_features/resolve/main/validation_set_features.npy

# ========== 插入数据准备代码 ==========
print("\n--- Starting Data Preparation ---")

# --- 定义参数 ---
# dir_custom_data 已在脚本开头定义
output_base_dir = "./my_custom_model"
model_name = target_word.replace(" ", "_")
train_split_ratio = 0.8 # 训练集比例
target_positive_count = 1000 # <<< 新增：目标正样本数量

# --- 定义源文件模式 ---
positive_files_pattern = os.path.join(dir_cut_syl, "a.*.wav") # 正样本
negative_b_pattern = os.path.join(dir_cut_syl, "b.*.wav")     # 负样本 b
negative_c_pattern = os.path.join(dir_cut_syl, "c.*.wav")     # 负样本 c

# --- 定义目标目录 ---
target_dir = os.path.join(output_base_dir, model_name)
pos_train_dir = os.path.join(target_dir, "positive_train")
pos_test_dir = os.path.join(target_dir, "positive_test")
neg_train_dir = os.path.join(target_dir, "negative_train")
neg_test_dir = os.path.join(target_dir, "negative_test")

# --- 清理并创建目标目录 ---
if os.path.exists(target_dir):
    print(f"Cleaning existing target directory: {target_dir}")
    shutil.rmtree(target_dir)
print(f"Creating target directories under: {target_dir}")
os.makedirs(pos_train_dir, exist_ok=True)
os.makedirs(pos_test_dir, exist_ok=True)
os.makedirs(neg_train_dir, exist_ok=True)
os.makedirs(neg_test_dir, exist_ok=True)

# --- 获取并排序源正样本文件 ---
print(f"\nProcessing source positive files from pattern '{positive_files_pattern}'...")
source_positive_files = glob.glob(positive_files_pattern)

# 辅助函数：从文件名提取数字用于排序
def get_file_number(filename):
    try:
        # 提取文件名中的数字部分，例如 'b.123.wav' -> 123
        return int(os.path.basename(filename).split('.')[1])
    except (IndexError, ValueError):
        # 如果文件名格式不匹配，返回一个大数或处理错误
        print(f"Warning: Could not extract number from filename {filename}, using infinity for sort key.")
        return float('inf')

source_positive_files.sort(key=get_file_number) # 按数字排序
num_source_files = len(source_positive_files)
print(f"Found and sorted {num_source_files} source positive files (e.g., '{os.path.basename(source_positive_files[0]) if num_source_files > 0 else 'N/A'}').")

num_pos_train = 0
num_pos_test = 0

if num_source_files == 0:
    print(f"警告：在 '{positive_files_pattern}' 未找到匹配的正样本文件。无法生成目标文件。")
else:
    # --- 生成目标文件名列表 (1 到 target_positive_count) ---
    target_filenames = [f"a.{i}.wav" for i in range(1, target_positive_count + 1)] # <<< 修改这里
    random.shuffle(target_filenames) # 打乱目标顺序

    # --- 分割目标文件名 ---
    n_total_target = len(target_filenames)
    n_train_target = math.ceil(n_total_target * train_split_ratio)
    train_target_filenames = target_filenames[:n_train_target]
    test_target_filenames = target_filenames[n_train_target:]
    print(f"Targeting {len(train_target_filenames)} training files and {len(test_target_filenames)} test files (total {target_positive_count}).")

    # --- 复制和重命名生成训练集 ---
    print(f"\nGenerating positive training files in {pos_train_dir}...")
    for target_filename in tqdm(train_target_filenames, desc="Generating train files"):
        try:
            target_number = get_file_number(target_filename)
            if target_number == float('inf'): continue # 跳过无法解析的目标名

            source_file_index = (target_number - 1) % num_source_files # 计算源文件索引 (0-based)
            source_file_path = source_positive_files[source_file_index]
            target_file_path = os.path.join(pos_train_dir, target_filename)
            shutil.copy(source_file_path, target_file_path)
            num_pos_train += 1
        except Exception as e:
            print(f"    Error generating {target_filename} for training set: {e}")

    # --- 复制和重命名生成测试集 ---
    print(f"\nGenerating positive test files in {pos_test_dir}...")
    for target_filename in tqdm(test_target_filenames, desc="Generating test files"):
        try:
            target_number = get_file_number(target_filename)
            if target_number == float('inf'): continue # 跳过无法解析的目标名

            source_file_index = (target_number - 1) % num_source_files # 计算源文件索引 (0-based)
            source_file_path = source_positive_files[source_file_index]
            target_file_path = os.path.join(pos_test_dir, target_filename)
            shutil.copy(source_file_path, target_file_path)
            num_pos_test += 1
        except Exception as e:
            print(f"    Error generating {target_filename} for test set: {e}")

print(f"\nActual positive files generated: Train={num_pos_train}, Test={num_pos_test}")

# --- 获取并处理源负样本文件 ---
print(f"\nProcessing source negative files from '{dir_cut_syl}'...")
source_negative_files_b = glob.glob(negative_b_pattern)
source_negative_files_c = glob.glob(negative_c_pattern)
all_source_negative_files = source_negative_files_b + source_negative_files_c
random.shuffle(all_source_negative_files) # 打乱所有收集到的负样本文件

num_source_negative_files = len(all_source_negative_files)
print(f"Found {len(source_negative_files_b)} 'b.*.wav' files and {len(source_negative_files_c)} 'c.*.wav' files. Total {num_source_negative_files} source negative files.")

num_neg_train = 0
num_neg_test = 0

if num_source_negative_files == 0:
    print(f"警告：在 '{dir_cut_syl}' 未找到匹配的 'b.*.wav' 或 'c.*.wav' 负样本文件。将创建虚拟负样本。")
    # 如果没有找到真实负样本，仍然创建虚拟负样本以避免后续流程出错
    if num_pos_train > 0 or num_pos_test > 0: # 只有在有正样本时才创建虚拟负样本
        print("\nCreating dummy negative files as fallback...")
        sampling_rate = 16000
        duration_ms = 50
        silence = np.zeros(int(sampling_rate * duration_ms / 1000), dtype=np.int16)

        dummy_neg_train_file_path = os.path.join(neg_train_dir, "dummy_negative.wav")
        scipy.io.wavfile.write(dummy_neg_train_file_path, sampling_rate, silence)
        print(f"Dummy negative train file created: {dummy_neg_train_file_path}")
        num_neg_train = 1 # 计数为1

        dummy_neg_test_file_path = os.path.join(neg_test_dir, "dummy_negative.wav")
        scipy.io.wavfile.write(dummy_neg_test_file_path, sampling_rate, silence)
        print(f"Dummy negative test file created: {dummy_neg_test_file_path}")
        num_neg_test = 1 # 计数为1
else:
    n_neg_train_target = math.ceil(num_source_negative_files * train_split_ratio)
    train_negative_source_files = all_source_negative_files[:n_neg_train_target]
    test_negative_source_files = all_source_negative_files[n_neg_train_target:]

    print(f"Targeting {len(train_negative_source_files)} negative training files and {len(test_negative_source_files)} negative test files.")

    # --- 复制生成负训练集 ---
    print(f"\nGenerating negative training files in {neg_train_dir}...")
    for source_file_path in tqdm(train_negative_source_files, desc="Generating neg train files"):
        try:
            target_file_name = os.path.basename(source_file_path) # 保留原始文件名
            target_file_path = os.path.join(neg_train_dir, target_file_name)
            shutil.copy(source_file_path, target_file_path)
            num_neg_train += 1
        except Exception as e:
            print(f"    Error copying {target_file_name} for negative training set: {e}")

    # --- 复制生成负测试集 ---
    print(f"\nGenerating negative test files in {neg_test_dir}...")
    for source_file_path in tqdm(test_negative_source_files, desc="Generating neg test files"):
        try:
            target_file_name = os.path.basename(source_file_path) # 保留原始文件名
            target_file_path = os.path.join(neg_test_dir, target_file_name)
            shutil.copy(source_file_path, target_file_path)
            num_neg_test += 1
        except Exception as e:
            print(f"    Error copying {target_file_name} for negative test set: {e}")

print(f"\nActual negative files generated: Train={num_neg_train}, Test={num_neg_test}")

print("\nData preparation finished.")
print("-" * 30 + "\n")

# ========== 添加检查点 ==========
print("\n--- Checking prepared files ---")
print(f"Positive train files in {pos_train_dir}:")
!ls -1 {pos_train_dir} | wc -l
print(f"\nPositive test files in {pos_test_dir}:")
!ls -1 {pos_test_dir} | wc -l
# 检查 neg_train (如果创建了虚拟文件，应该至少有1个)
print(f"\nNegative train files in {neg_train_dir}:")
!ls -1 {neg_train_dir} | wc -l
# === 修改：检查 neg_test (如果创建了虚拟文件，应该至少有1个) ===
print(f"\nNegative test files in {neg_test_dir}:")
!ls -1 {neg_test_dir} | wc -l
# === 结束修改 ===
print("-" * 30 + "\n")
# ========== 结束检查点 ==========


# @title  { display-mode: "form" }
# @markdown # 3. Train the Model
# @markdown Now that the data has been prepared and downloaded,
# @markdown the last step is to adjust the training paramaters (or keep
# @markdown the defaults below) and start the training!
# @markdown Note that the number of examples is now determined by the files copied in the previous step.

# @markdown Each paramater controls a different aspect of training:
# @markdown - `number_of_training_steps` controls how long to train the model.
# @markdown The default (10,000) usually works well but training longer usually helps.

# @markdown - `false_activation_penalty` controls how strongly false activations
# @markdown are penalized during the training process. Higher values can make the model
# @markdown much less likely to activate when it shouldn't, but may also cause it
# @markdown to not activate when the wake word isn't spoken clearly and there is
# @markdown background noise.

# @markdown With the default values shown below,
# @markdown this takes about 30 - 60 minutes total on the normal CPU Colab runtime.
# @markdown If you want to train for longer,
# @markdown try changing the runtime type to a GPU to significantly speedup
# @markdown the model training.

# @markdown When the model finishes training, you can navigate to the `my_custom_model` folder
# @markdown in the file browser on the left (click on the folder icon), and download
# @markdown the [your target wake word].onnx or  <your target wake word>.tflite files.
# @markdown You can then use these as you would any other openWakeWord model!

# Load default YAML config file for training
import yaml
print("Loading default training config...")
config = yaml.load(open("openwakeword/examples/custom_model.yml", 'r').read(), yaml.Loader)
print("Default config loaded.")

# Modify values in the config and save a new version
# number_of_examples = 1000 # @param {type:"slider", min:100, max:50000, step:50} # 不再使用滑块设置
number_of_training_steps = 10000  # @param {type:"slider", min:0, max:50000, step:100}
false_activation_penalty = 1500  # @param {type:"slider", min:100, max:5000, step:50}
# config["target_phrase"] = [target_word] # 不再需要设置 target_phrase，train.py 会从目录结构推断
config["model_name"] = model_name # 使用上面数据准备部分确定的 model_name
# config["n_samples"] = number_of_examples # 使用实际分割后的正样本训练数
if num_pos_train > 0:
    config["n_samples"] = num_pos_train
    print(f"Setting n_samples (training positive examples) to {num_pos_train}")
else:
    print("Warning: num_pos_train is 0. Using default n_samples from config if available, or training might fail.")
    # 可以选择保留默认值或设置为一个小的非零值以允许脚本运行，但可能效果不佳
    # config["n_samples"] = config.get("n_samples", 10) # 保留默认或设为 10

# config["n_samples_val"] = max(500, number_of_examples//10) # 使用实际分割后的正样本验证数
if num_pos_test > 0:
    config["n_samples_val"] = num_pos_test
    print(f"Setting n_samples_val (validation positive examples) to {num_pos_test}")
else:
    # 如果没有测试样本，验证可能会有问题，可以设为0或1，或依赖 train.py 的处理
    config["n_samples_val"] = 0
    print("Warning: num_pos_test is 0. Setting n_samples_val to 0.")

config["steps"] = number_of_training_steps
config["target_accuracy"] = 0.5
config["target_recall"] = 0.25
config["output_dir"] = output_base_dir # 使用上面定义的基础输出目录
config["max_negative_weight"] = false_activation_penalty

# train.py 会自动查找 {output_dir}/{model_name}/{positive_train, positive_test, negative_train, negative_test}
# 确保这些目录存在（已在数据准备阶段创建）

config["background_paths"] = ['./audioset_16k', './fma']  # 保留背景噪音数据路径
config["false_positive_validation_data_path"] = "validation_set_features.npy" # 保留预计算特征路径
config["feature_data_files"] = {"ACAV100M_sample": "openwakeword_features_ACAV100M_2000_hrs_16bit.npy"} # 保留预计算特征路径

# 打印最终配置以供检查
print("\nFinal Training Configuration:")
print(yaml.dump(config))
print("-" * 30)

with open('my_model.yaml', 'w') as file:
    documents = yaml.dump(config, file)
print("Saved training configuration to my_model.yaml")

# Generate clips (不再需要)
# !{sys.executable} openwakeword/openwakeword/train.py --training_config my_model.yaml --generate_clips
print("\nSkipping synthetic clip generation step as positive examples were copied.")

# Step 2: Augment the copied positive clips
print("\nStarting augmentation step...")
!{sys.executable} openwakeword/openwakeword/train.py --training_config my_model.yaml --augment_clips

# Step 3: Train model
print("\nStarting model training step...")
!{sys.executable} openwakeword/openwakeword/train.py --training_config my_model.yaml --train_model

# # Manually save to tflite as this doesn't work right in colab (broken in python 3.11, default in Colab as of January 2025)
# def convert_onnx_to_tflite(onnx_model_path, output_path):
# ... existing code ...

# Convert ONNX model to tflite using `onnx2tf` library (works for python 3.11 as of January 2025)
onnx_model_path = f"my_custom_model/{config['model_name']}.onnx"
name1, name2 = f"my_custom_model/{config['model_name']}_float32.tflite", f"my_custom_model/{config['model_name']}.tflite"
!onnx2tf -i {onnx_model_path} -o my_custom_model/ -kat onnx____Flatten_0
!mv {name1} {name2}

# Automatically download the trained model files
from google.colab import files

files.download(f"my_custom_model/{config['model_name']}.onnx")
files.download(f"my_custom_model/{config['model_name']}.tflite")

