# -*- coding: utf-8 -*-
"""openwakeword_automatic_model_training_simple.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gDfd3kcsxgrHxHCJsz8rE7_d9Up9VQhF
"""
import sys
from google.colab import drive
drive.mount('/content/gdrive', force_remount=True)
dir_cut_syl="/content/gdrive/MyDrive/ColabData/openWakeWord/cut_syl_data"
!ls -dal $dir_cut_syl

!ls -al $dir_cut_syl | wc -l

# "Edit / Notebook settings / Omit code cell output when saving this notebook", then "File / Download / Download .ipynb"
!rm -rf my_custom_model/hey*
!rm -rf my_custom_model/heh*
!ls
#!pip uninstall torchaudio
!find . -type d -name '__pycache__' -exec rm -rf {} +
!pip install torch==2.5.1 torchvision==0.20.1+cu121 torchaudio==2.5.1+cu121 --index-url https://download.pytorch.org/whl/cu121
!pip list | grep -E "torch|torchaudio|piper|webrtcvad"
!nvcc --version

import torch
print("CUDA available:", torch.cuda.is_available())
print("PyTorch CUDA version:", torch.version.cuda)
print("Current device:", torch.cuda.current_device())
print("Device name:", torch.cuda.get_device_name(0))

import torchaudio
print(torchaudio.__file__)
print("TorchAudio version:", torchaudio.__version__)
print("CUDA version from TorchAudio:", torchaudio.lib._torchaudio.cuda_version())

"""## Training your own openWakeWord models

**Quick-start:** If you just want to train a basic custom model for openWakeWord!

Follow the instructions for Step 1 below. Each time you change the wake word, click the play icon to the left of the title to generate a sample and make sure it sounds correct. The first time it takes ~30 seconds but subsequent runs will be quick.

Once you're satisfied with the pronounciation, go to the "Runtime" dropdown menu in the upper left of the page, and select "run all". Keep the tab open but feel free to do something else. After ~1 hour, your custom model will be ready and will automatically be downloaded to your computer!

If you are a Home Assistant user with the openWakeWord add-on, follow the instructions [here](https://github.com/home-assistant/addons/blob/master/openwakeword/DOCS.md#custom-wake-word-models) to install and enable your custom model.

---

If you are interested in learning more about the custom model training process (and increasing the accuracy of your custom models), read through each step in this notebook and try experimenting with different training parameters. If you have any questions or problems, feel free to start a discussion at the openWakeWord [repo](https://github.com/dscripka/openWakeWord/discussions).
"""

# @title  { display-mode: "form" }
# @markdown # 1. Test Example Training Clip Generation
# @markdown Since openWakeWord models are trained on synthetic examples of your
# @markdown target wake word, it's a good idea to make sure that the examples
# @markdown sound correct. Type in your target wake word below, and run the
# @markdown cell to listen to it.
# @markdown
# @markdown Here are some tips that can help get the wake word to sound right:

# @markdown - If your wake word isn't being pronounced in the way
# @markdown you want, try spelling out the sounds phonetically with underscores
# @markdown separating each part.
# @markdown For example: "hey siri" --> "hey_seer_e".

# @markdown - Spell out numbers ("2" --> "two")

# @markdown - Avoid all punctuation except for "?" and "!", and remove unicode characters
# @markdown - https://github.com/alphacep/vosk-api/issues/730
# @markdown - "eigh", "bee", "see", "dee", "yee", "fay", "jee" | "hay", "eye", "jay", "kay", "lie", "more", "knee"
# @markdown - "oh", "pee", "queue", "are", "say", "tee" |  "you", "we", "woo", "core", "why", "zee"

import os
import sys
from IPython.display import Audio
# # ==== 注释掉测试生成 ====
if not os.path.exists("./piper-sample-generator"):
    !git clone https://github.com/rhasspy/piper-sample-generator
#     !wget -O piper-sample-generator/models/en_US-libritts_r-medium.pt 'https://github.com/rhasspy/piper-sample-generator/releases/download/v2.0.0/en_US-libritts_r-medium.pt'
#
# # Install system dependencies
!pip install piper-phonemize
!pip install webrtcvad
!pip install onnxsim
#

#needed by train.py, even if won't use this:
if "piper-sample-generator/" not in sys.path:
    sys.path.append("piper-sample-generator/")
from generate_samples import generate_samples
#
# target_word = 'hay' # @param {type:"string"} # 这个变量后面会被覆盖，但暂时保留以防万一
#
# def text_to_speech(text):
#     generate_samples(text = text,
#                 max_samples=1,
#                 length_scales=[1.1],
#                 noise_scales=[0.7], noise_scale_ws = [0.7],
#                 output_dir = './', batch_size=1, auto_reduce_batch_size=True,
#                 file_names=["test_generation.wav"]
#                 )
#
# text_to_speech(target_word)
# Audio("test_generation.wav", autoplay=True)
# # ==== 结束注释 ====

# @title  { display-mode: "form" }
# @markdown # 2. Download Data
# @markdown Training custom models requires downloading a wide variety of data
# @markdown that will help make the model perform well in real-world scenarios.
# @markdown This example notebook will download small samples of background noise,
# @markdown music, and Room Impulse Responses (to add echo). This will still produce
# @markdown a custom model that performs well, but if you are interested in adding even more,
# @markdown feel free to extend this notebook to download the full datasets and even add
# @markdown your own!
# @markdown
# @markdown Downloading this example data will usually take about 15 minutes.

# @markdown **Important note!** The data downloaded here has a mixture of difference
# @markdown licenses and usage restrictions. As such, any custom models trained with this
# @markdown data should be considered as appropriate for **non-commercial** personal use only.

# ## Install all dependencies
# !pip install datasets
# !pip install scipy
# !pip install tqdm

import locale
def getpreferredencoding(do_setlocale = True):
    return "UTF-8"
locale.getpreferredencoding = getpreferredencoding

# install openwakeword (full installation to support training)
!git clone https://github.com/diyism/openwakeword
!pip install -e ./openwakeword
!cd openwakeword

print("==================before some pip installs")
!ls -dal $dir_cut_syl

# install other dependencies
!pip install mutagen==1.47.0
!pip install torchinfo==1.8.0
!pip install torchmetrics==1.2.0
!pip install speechbrain==0.5.14
!pip install audiomentations==0.33.0
!pip install torch-audiomentations==0.11.0
!pip install acoustics==0.2.6
# !pip uninstall tensorflow -y
# !pip install tensorflow-cpu==2.8.1
# !pip install protobuf==3.20.3
# !pip install tensorflow_probability==0.16.0
# !pip install onnx_tf==1.10.0
!pip install onnx2tf
!pip install onnx
!pip install onnx_graphsurgeon
!pip install sng4onnx
!pip install pronouncing==0.2.0
!pip install datasets==2.14.6
!pip install deep-phonemizer==0.0.19

print("Installing dependencies for model conversion...")
!pip install onnx-tf # 满足 train.py 内部转换函数的需求
!pip install ai-edge-litert # 满足 onnx2tf 命令行的需求
print("Model conversion dependencies installation attempted.")

!ls -dal $dir_cut_syl

# Download required models (workaround for Colab)
import os
#os.makedirs("./openwakeword/openwakeword/resources/models")
!wget https://github.com/dscripka/openWakeWord/releases/download/v0.5.1/embedding_model.onnx -O ./openwakeword/openwakeword/resources/models/embedding_model.onnx
!wget https://github.com/dscripka/openWakeWord/releases/download/v0.5.1/embedding_model.tflite -O ./openwakeword/openwakeword/resources/models/embedding_model.tflite
!wget https://github.com/dscripka/openWakeWord/releases/download/v0.5.1/melspectrogram.onnx -O ./openwakeword/openwakeword/resources/models/melspectrogram.onnx
!wget https://github.com/dscripka/openWakeWord/releases/download/v0.5.1/melspectrogram.tflite -O ./openwakeword/openwakeword/resources/models/melspectrogram.tflite


import numpy as np
import torch
import sys
from pathlib import Path
import uuid
import yaml
import datasets
import scipy
from tqdm import tqdm

## Download all data

## Download MIR RIR data (takes about ~2 minutes)
output_dir = "./mit_rirs"
if not os.path.exists(output_dir):
    os.mkdir(output_dir)
    !git lfs install
    !git clone https://huggingface.co/datasets/davidscripka/MIT_environmental_impulse_responses
    rir_dataset = datasets.Dataset.from_dict({"audio": [str(i) for i in Path("./MIT_environmental_impulse_responses/16khz").glob("*.wav")]}).cast_column("audio", datasets.Audio())
    # Save clips to 16-bit PCM wav files
    for row in tqdm(rir_dataset):
        name = row['audio']['path'].split('/')[-1]
        scipy.io.wavfile.write(os.path.join(output_dir, name), 16000, (row['audio']['array']*32767).astype(np.int16))

## Download noise and background audio (takes about ~3 minutes)

# Audioset Dataset (https://research.google.com/audioset/dataset/index.html)
# Download one part of the audioset .tar files, extract, and convert to 16khz
# For full-scale training, it's recommended to download the entire dataset from
# https://huggingface.co/datasets/agkphysics/AudioSet, and
# even potentially combine it with other background noise datasets (e.g., FSD50k, Freesound, etc.)

if not os.path.exists("audioset"):
    os.mkdir("audioset")

    fname = "bal_train09.tar"
    out_dir = f"audioset/{fname}"
    link = "https://huggingface.co/datasets/agkphysics/AudioSet/resolve/main/data/" + fname
    !wget -O {out_dir} {link}
    !cd audioset && tar -xvf bal_train09.tar

    output_dir = "./audioset_16k"
    if not os.path.exists(output_dir):
        os.mkdir(output_dir)

    # Save clips to 16-bit PCM wav files
    audioset_dataset = datasets.Dataset.from_dict({"audio": [str(i) for i in Path("audioset/audio").glob("**/*.flac")]})
    audioset_dataset = audioset_dataset.cast_column("audio", datasets.Audio(sampling_rate=16000))
    for row in tqdm(audioset_dataset):
        name = row['audio']['path'].split('/')[-1].replace(".flac", ".wav")
        scipy.io.wavfile.write(os.path.join(output_dir, name), 16000, (row['audio']['array']*32767).astype(np.int16))

# Free Music Archive dataset
# https://github.com/mdeff/fma

output_dir = "./fma"
if not os.path.exists(output_dir):
    os.mkdir(output_dir)
    fma_dataset = datasets.load_dataset("rudraml/fma", name="small", split="train", streaming=True)
    fma_dataset = iter(fma_dataset.cast_column("audio", datasets.Audio(sampling_rate=16000)))

    # Save clips to 16-bit PCM wav files
    n_hours = 1  # use only 1 hour of clips for this example notebook, recommend increasing for full-scale training
    for i in tqdm(range(n_hours*3600//30)):  # this works because the FMA dataset is all 30 second clips
        row = next(fma_dataset)
        name = row['audio']['path'].split('/')[-1].replace(".mp3", ".wav")
        scipy.io.wavfile.write(os.path.join(output_dir, name), 16000, (row['audio']['array']*32767).astype(np.int16))
        i += 1
        if i == n_hours*3600//30:
            break

# Download pre-computed openWakeWord features for training and validation

# training set (~2,000 hours from the ACAV100M Dataset)
# See https://huggingface.co/datasets/davidscripka/openwakeword_features for more information
if not os.path.exists("./openwakeword_features_ACAV100M_2000_hrs_16bit.npy"):
    !wget https://huggingface.co/datasets/davidscripka/openwakeword_features/resolve/main/openwakeword_features_ACAV100M_2000_hrs_16bit.npy

# validation set for false positive rate estimation (~11 hours)
if not os.path.exists("validation_set_features.npy"):
    !wget https://huggingface.co/datasets/davidscripka/openwakeword_features/resolve/main/validation_set_features.npy

# ========== before training, clear unfinished/dirty files:
# !ls -al my_custom_model/hay/negative_train/ | wc -l  # 不再需要针对 'hay' 清理
# !rm -f my_custom_model/hay/negative_train/*           # 不再需要针对 'hay' 清理
# !ls -al my_custom_model/hay/negative_train/ | wc -l  # 不再需要针对 'hay' 清理
#
# !ls -al my_custom_model/hay/negative_test/ | wc -l   # 不再需要针对 'hay' 清理
# !rm -f my_custom_model/hay/negative_test/*            # 不再需要针对 'hay' 清理
# !ls -al my_custom_model/hay/negative_test/ | wc -l   # 不再需要针对 'hay' 清理

# =========== before training, modify content/openwakeword/examples/custom_model.yaml, set custom_negative_phrases(at no.13 line),
# its configs will be copied into my_model.yaml
# the values is in the 1st step: "eigh", "bee", "see", ..., but get rid of current target_word
# =========== 不再需要修改 custom_negative_phrases，因为我们将使用提供的负样本文件


# ========== 插入数据准备代码 ==========
import os
import glob
import random
import shutil
import math
import yaml # 确保 yaml 已导入

# --- 定义参数 ---
# !!! 重要：请将下面的路径修改为您存放 a.*.wav, b.*.wav, c.*.wav 的实际路径 !!!
dir_custom_data = dir_cut_syl # <--- 修改这里

output_base_dir = "./my_custom_model"
# 使用固定的模型名称，因为我们不再基于 target_word 生成
model_name = "wakeword_a"
train_split_ratio = 0.6

# --- 定义源文件模式 ---
positive_files_pattern = os.path.join(dir_custom_data, "a.*.wav")
negative_files_patterns = [
    os.path.join(dir_custom_data, "b.*.wav"),
    os.path.join(dir_custom_data, "c.*.wav")
]

# --- 定义目标目录 ---
target_dir = os.path.join(output_base_dir, model_name)
pos_train_dir = os.path.join(target_dir, "positive_train")
pos_test_dir = os.path.join(target_dir, "positive_test")
neg_train_dir = os.path.join(target_dir, "negative_train")
neg_test_dir = os.path.join(target_dir, "negative_test")

# --- 清理并创建目标目录 ---
if os.path.exists(target_dir):
    print(f"Cleaning existing target directory: {target_dir}")
    shutil.rmtree(target_dir)
print(f"Creating target directories under: {target_dir}")
os.makedirs(pos_train_dir, exist_ok=True)
os.makedirs(pos_test_dir, exist_ok=True)
os.makedirs(neg_train_dir, exist_ok=True)
os.makedirs(neg_test_dir, exist_ok=True)

# --- 用于分割和复制文件的函数 (修改版：转换采样率) ---
def split_and_copy_files(file_list, train_dir, test_dir, ratio):
    if not file_list:
        return 0, 0
    random.shuffle(file_list)
    n_total = len(file_list)
    n_train = math.ceil(n_total * ratio)
    train_files = file_list[:n_train]
    test_files = file_list[n_train:]

    print(f"  Copying {len(train_files)} files to {train_dir}")
    for f in train_files:
        try:
            shutil.copy(f, os.path.join(train_dir, os.path.basename(f)))
        except Exception as e:
            print(f"    Error copying {f} to {train_dir}: {e}")
    print(f"  Copying {len(test_files)} files to {test_dir}")
    for f in test_files:
         try:
            shutil.copy(f, os.path.join(test_dir, os.path.basename(f)))
         except Exception as e:
            print(f"    Error copying {f} to {test_dir}: {e}")
    return len(train_files), len(test_files)

# --- 处理正样本文件 ---
print(f"Processing positive files from '{positive_files_pattern}'...")
positive_files = glob.glob(positive_files_pattern)
print(f"Found {len(positive_files)} positive files.")
if not positive_files:
    print(f"警告：在 '{positive_files_pattern}' 未找到正样本文件。训练可能会失败。")
num_pos_train, num_pos_test = split_and_copy_files(positive_files, pos_train_dir, pos_test_dir, train_split_ratio)
print(f"Actual positive files copied: Train={num_pos_train}, Test={num_pos_test}")


# --- 处理负样本文件 ---
print("Processing negative files...")
negative_files = []
for pattern in negative_files_patterns:
    print(f"  Checking pattern: '{pattern}'")
    found_files = glob.glob(pattern)
    print(f"  Found {len(found_files)} files.")
    negative_files.extend(found_files)

print(f"Total negative files found: {len(negative_files)}")
if not negative_files:
     print(f"警告：未找到匹配的负样本文件。训练可能会失败。")
num_neg_train, num_neg_test = split_and_copy_files(negative_files, neg_train_dir, neg_test_dir, train_split_ratio)
print(f"Actual negative files copied: Train={num_neg_train}, Test={num_neg_test}")

print("Data preparation finished.")

# ========== 添加检查点 ==========
print("\n--- Checking prepared positive files ---")
print(f"Checking contents of positive train directory: {pos_train_dir}")
!ls -lh {pos_train_dir} | head -n 10
!echo "Total files in {pos_train_dir}: $(ls -1 {pos_train_dir} | wc -l)"

print(f"\nChecking contents of positive test directory: {pos_test_dir}")
!ls -lh {pos_test_dir} | head -n 10
!echo "Total files in {pos_test_dir}: $(ls -1 {pos_test_dir} | wc -l)"
print("-" * 30 + "\n")
# ========== 结束检查点 ==========


# @title  { display-mode: "form" }
# @markdown # 3. Train the Model
# @markdown Now that you have verified your target wake word and downloaded the data,
# @markdown the last step is to adjust the training paramaters (or keep
# @markdown the defaults below) and start the training!

# @markdown Each paramater controls a different aspect of training:
# @markdown - `number_of_examples` controls how many examples of your wakeword
# @markdown are generated. The default (1,000) usually produces a good model,
# @markdown but between 30,000 and 50,000 is often the best.

# @markdown - `number_of_training_steps` controls how long to train the model.
# @markdown Similar to the number of examples, the default (10,000) usually works well
# @markdown but training longer usually helps.

# @markdown - `false_activation_penalty` controls how strongly false activations
# @markdown are penalized during the training process. Higher values can make the model
# @markdown much less likely to activate when it shouldn't, but may also cause it
# @markdown to not activate when the wake word isn't spoken clearly and there is
# @markdown background noise.

# @markdown With the default values shown below,
# @markdown this takes about 30 - 60 minutes total on the normal CPU Colab runtime.
# @markdown If you want to train on more examples or train for longer,
# @markdown try changing the runtime type to a GPU to significantly speedup
# @markdown the example generating and model training.

# @markdown When the model finishes training, you can navigate to the `my_custom_model` folder
# @markdown in the file browser on the left (click on the folder icon), and download
# @markdown the [your target wake word].onnx or  <your target wake word>.tflite files.
# @markdown You can then use these as you would any other openWakeWord model!

# Load default YAML config file for training
import yaml
config = yaml.load(open("openwakeword/examples/custom_model.yml", 'r').read(), yaml.Loader)

# Modify values in the config and save a new version
# number_of_examples = 1000 # @param {type:"slider", min:100, max:50000, step:50} # 不再使用滑块设置，由实际文件数决定
number_of_training_steps = 10000  # @param {type:"slider", min:0, max:50000, step:100}
false_activation_penalty = 1500  # @param {type:"slider", min:100, max:5000, step:50}
# config["target_phrase"] = [target_word] # 不再基于 target_word 生成
config["model_name"] = model_name # 使用上面定义的数据准备部分的 model_name
# config["n_samples"] = number_of_examples # 使用实际分割后的正样本训练数
config["n_samples"] = num_pos_train # 使用实际分割后的正样本训练数
# config["n_samples_val"] = max(500, number_of_examples//10) # 使用实际分割后的正样本验证数
config["n_samples_val"] = num_pos_test # 使用实际分割后的正样本验证数
config["steps"] = number_of_training_steps
config["target_accuracy"] = 0.5
config["target_recall"] = 0.25
config["output_dir"] = output_base_dir # 使用上面定义的基础输出目录
config["max_negative_weight"] = false_activation_penalty

# 指定包含正负样本的目录，train.py 会自动查找
# train.py 脚本似乎会根据 output_dir 和 model_name 自动查找 {output_dir}/{model_name}/{positive_train, positive_val, negative_train, negative_test}
# 所以不需要显式设置 positive_data_paths 或 negative_data_paths

config["background_paths"] = ['./audioset_16k', './fma']  # 保留背景噪音数据路径
config["false_positive_validation_data_path"] = "validation_set_features.npy" # 保留预计算特征路径
config["feature_data_files"] = {"ACAV100M_sample": "openwakeword_features_ACAV100M_2000_hrs_16bit.npy"} # 保留预计算特征路径

# # 在保存配置前，打印最终配置以供检查
# print("\nFinal Training Configuration:")
# print(yaml.dump(config))
# print("-" * 30)


with open('my_model.yaml', 'w') as file:
    documents = yaml.dump(config, file)
print("Saved training configuration to my_model.yaml")

# Generate clips
# !{sys.executable} openwakeword/openwakeword/train.py --training_config my_model.yaml --generate_clips # ==== 注释掉：不再生成剪辑 ====
print("Skipping synthetic clip generation step.")

# Step 2: Augment the generated clips
print("Starting augmentation step...") # 添加打印语句
!{sys.executable} openwakeword/openwakeword/train.py --training_config my_model.yaml --augment_clips

# Step 3: Train model
print("Starting model training step...") # 添加打印语句
!{sys.executable} openwakeword/openwakeword/train.py --training_config my_model.yaml --train_model

# # Manually save to tflite as this doesn't work right in colab (broken in python 3.11, default in Colab as of January 2025)
# def convert_onnx_to_tflite(onnx_model_path, output_path):
#     """Converts an ONNX version of an openwakeword model to the Tensorflow tflite format."""
#     # imports
#     import onnx
#     import logging
#     import tempfile
#     from onnx_tf.backend import prepare
#     import tensorflow as tf

#     # Convert to tflite from onnx model
#     onnx_model = onnx.load(onnx_model_path)
#     tf_rep = prepare(onnx_model, device="CPU")
#     with tempfile.TemporaryDirectory() as tmp_dir:
#         tf_rep.export_graph(os.path.join(tmp_dir, "tf_model"))
#         converter = tf.lite.TFLiteConverter.from_saved_model(os.path.join(tmp_dir, "tf_model"))
#         tflite_model = converter.convert()

#         logging.info(f"####\nSaving tflite mode to '{output_path}'")
#         with open(output_path, 'wb') as f:
#             f.write(tflite_model)

#     return None

# convert_onnx_to_tflite(f"my_custom_model/{config['model_name']}.onnx", f"my_custom_model/{config['model_name']}.tflite")

# Convert ONNX model to tflite using `onnx2tf` library (works for python 3.11 as of January 2025)
onnx_model_path = f"my_custom_model/{config['model_name']}.onnx"
# 定义 onnx2tf 的输出路径（它会在 -o 指定目录下创建子目录）
onnx2tf_output_dir = f"my_custom_model/saved_model_{config['model_name']}"
# 源 tflite 文件路径
tflite_source_path = f"{onnx2tf_output_dir}/{config['model_name']}_float32.tflite"
# 目标 tflite 文件路径
tflite_target_path = f"my_custom_model/{config['model_name']}.tflite"

print("Running ONNX to TFLite conversion...")
!onnx2tf -i {onnx_model_path} -o my_custom_model/ -kat onnx____Flatten_0

# 检查源文件是否存在再移动
print(f"Attempting to move {tflite_source_path} to {tflite_target_path}")
if os.path.exists(tflite_source_path):
    !mv {tflite_source_path} {tflite_target_path}
    print("Move successful.")
else:
    print(f"Error: Source TFLite file not found at {tflite_source_path}. Check onnx2tf output.")
    # 如果需要，可以尝试查找其他可能的 tflite 文件名
    # !ls -l {onnx2tf_output_dir}

# Automatically download the trained model files
from google.colab import files

files.download(f"my_custom_model/{config['model_name']}.onnx")
files.download(f"my_custom_model/{config['model_name']}.tflite")
